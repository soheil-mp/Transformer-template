{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h1 style=\"text-align:center\">Transformer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Initial Deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import datetime, os, sys, time, warnings, re, requests, json, pickle, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matplotlib style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "DATA_SHUFFLE = 1000\n",
    "DATA_REPEAT = 1\n",
    "MAX_TOKENS = 100000\n",
    "MAX_SEQ_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure GPU is available\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dataset Pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add <START>, <END> and <PAD> tags in the begining/end of sequences.\n",
    "# TODO: Finalize the dataset\n",
    "# TODO: Add an extra step for pre-training\n",
    "# TODO: Optimize the data loading steps\n",
    "# TODO: Bucketization step\n",
    "# TODO: Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a generator to read the data\n",
    "def data_generator():\n",
    "\n",
    "    # Loop over paths\n",
    "    for i_path in [\"./../dataset/glue_data/QNLI/train.tsv\"]:\n",
    "            \n",
    "        # Open the file\n",
    "        with open(i_path, mode='r', encoding='utf8', newline=\"\\n\") as file:\n",
    "            \n",
    "            # Loop over lines\n",
    "            for i_line in file:\n",
    "\n",
    "                # Ignore the first line\n",
    "                if i_line.startswith(\"index\"):  continue\n",
    "\n",
    "                # Split the line\n",
    "                i_line = i_line.split(\"\\t\")\n",
    "\n",
    "                # Set input/output\n",
    "                input_data = i_line[1]\n",
    "                output_data = i_line[2]\n",
    "\n",
    "                # Yield the line\n",
    "                yield (input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'When did the third Digimon series begin?'>, <tf.Tensor: shape=(), dtype=string, numpy=b'Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.'>)\n"
     ]
    }
   ],
   "source": [
    "# Data generator\n",
    "data_g = data_generator()\n",
    "\n",
    "# Convert to tf.data\n",
    "data = tf.data.Dataset.from_generator(\n",
    "    data_generator, \n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "        ),\n",
    "        )\n",
    "\n",
    "# Bucketize data\n",
    "# data = data.bucket_by_sequence_length(element_length_func=lambda elem: tf.shape(elem)[0],\n",
    "#                                       bucket_boundaries=[3, 5],\n",
    "#                                       bucket_BATCH_SIZEs=[2, 2, 2])\n",
    "\n",
    "# Print a sample\n",
    "for i_record in data.take(1):\n",
    "    print(i_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Function for custom standarization\n",
    "# TODO: Maybe use NLTK tokenizer\n",
    "# def custom_standardization(input_string):\n",
    "#     lowercased = tf.strings.lower(input_string)\n",
    "#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  82244\n"
     ]
    }
   ],
   "source": [
    "# Text vectorizer \n",
    "vectorize_layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation', \n",
    "                                                    split='whitespace', \n",
    "                                                    ngrams=None, \n",
    "                                                    output_mode='int', \n",
    "                                                    output_sequence_length=MAX_SEQ_LENGTH, \n",
    "                                                    pad_to_max_tokens=True, \n",
    "                                                    max_tokens=MAX_TOKENS, \n",
    "                                                    idf_weights=None, \n",
    "                                                    sparse=False, \n",
    "                                                    ragged=False)\n",
    "\n",
    "# Adapt the vectorizer to the data\n",
    "vectorize_layer.adapt(data)\n",
    "print(\"Vocabulary Size: \", vectorize_layer.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
      "array([  28,   17,    2,  354, 1989,  439,  362,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0], dtype=int64)>, <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
      "array([ 1697,     2,    60,  1573,   139,    24,     5,    43,     3,\n",
      "           2,  1573,    14,   622,  1989, 24030,  1760,     7,  9186,\n",
      "           5,    55,  9082,  1329,     6,    35,  1195,  2950,  1989,\n",
      "          22,    63,    36, 68666,    48,    38,  3001,     5,    55,\n",
      "         987,   765,   211,     4,     2,   357,   489,     0,     0,\n",
      "           0,     0,     0,     0,     0], dtype=int64)>)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data\n",
    "ds = data.map(lambda x, y: (vectorize_layer(x), vectorize_layer(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Print a sample\n",
    "for i in ds.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preparing dataset\n",
    "def prepare_dataset(in_record, out_record):\n",
    "\n",
    "    # Encoder input - actual input\n",
    "    encoder_input = in_record\n",
    "\n",
    "    # Decoder input - actual output (starting from 0 till t-1)\n",
    "    decoder_input = out_record[:-1] \n",
    "\n",
    "    # Add one zero to the end\n",
    "    decoder_input = tf.concat([decoder_input, [0]], axis=0)\n",
    "\n",
    "    # Decoder output - actual output (starting from 1 till t)\n",
    "    decoder_output = out_record[1:]\n",
    "\n",
    "    # Add one zero to the end\n",
    "    decoder_output = tf.concat([decoder_output, [0]], axis=0)\n",
    "\n",
    "    # Reshape output (to match sparse categorical crossentropy)\n",
    "    encoder_input = tf.expand_dims(encoder_input, axis=-1)\n",
    "    decoder_input = tf.expand_dims(decoder_input, axis=-1)\n",
    "    decoder_output = tf.expand_dims(decoder_output, axis=-1)\n",
    "\n",
    "    # Return the data\n",
    "    return ({\"encoder_inputs\": encoder_input, \"decoder_inputs\": decoder_input}, decoder_output,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "ds = ds.map(lambda in_record, out_record: prepare_dataset(in_record, out_record), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Print a sample\n",
    "# for i in ds.take(1):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "ds = ds.shuffle(DATA_SHUFFLE)\n",
    "#ds = ds.repeat(DATA_REPEAT)\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print a sample\n",
    "# for inputs, targets in ds.take(1):\n",
    "#     print(\"Encoder input: \", inputs[\"encoder_inputs\"].shape)\n",
    "#     print(\"Decoder input: \", inputs[\"decoder_inputs\"].shape)\n",
    "#     print(\"Decoder Output: \", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Report\n",
    "# for inputs, targets in ds.take(1):\n",
    "#     print(f'inputs[\"encoder_inputs\"]: \\n{inputs[\"encoder_inputs\"]}')\n",
    "#     print(f'\\ninputs[\"decoder_inputs\"]: \\n{inputs[\"decoder_inputs\"]}')\n",
    "#     print(f\"\\ntargets.shape: \\n{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Token and Positional Encoding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "# POSITIONAL ENCODING #\n",
    "#######################\n",
    "\n",
    "# Class for positional encoding\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "\n",
    "        # Inherite parent class constructor\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "        # Token and position embedding\n",
    "        self.token_embeddings    = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)        \n",
    "        self.position_embeddings = tf.keras.layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
    "        \n",
    "        # Initialization\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    # Call function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Length of inputs\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        \n",
    "        # Range of 0-length\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        \n",
    "        # Positional embedding\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        out = embedded_tokens + embedded_positions\n",
    "\n",
    "        return out \n",
    "\n",
    "    # Function for creating mask\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "\n",
    "        # Returns the truth value of (x != y) element-wise\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    # Function update parent's config\n",
    "    def get_config(self):\n",
    "\n",
    "        # Get the config of the parent class\n",
    "        config = super().get_config()\n",
    "\n",
    "        # Update the config\n",
    "        config.update({\"sequence_length\": self.sequence_length, \"vocab_size\": self.vocab_size, \"embed_dim\": self.embed_dim,})\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "# TRANSFORMER ENCODER #\n",
    "#######################\n",
    "\n",
    "# Class for transformer encoder\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "\n",
    "        # Inherite parent class constructor\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "\n",
    "        # Initialization\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Multi-head attention layer\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        # Dense projection layers\n",
    "        self.dense_proj = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                                               tf.keras.layers.Dense(embed_dim)])\n",
    "        \n",
    "        # Layer normalizations\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        # Flag for masking\n",
    "        self.supports_masking = True\n",
    "\n",
    "    # Call function\n",
    "    def call(self, inputs, mask=None):\n",
    "\n",
    "        # If mask is not None\n",
    "        if mask is not None:\n",
    "\n",
    "            # Mask the inputs\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        \n",
    "        # Feed into multi-head attention\n",
    "        attention_output = self.attention(query=inputs, value=inputs, key=inputs, attention_mask=padding_mask)\n",
    "        \n",
    "        # Sum up inputs and attention + Normalize the layer \n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "\n",
    "        # Feed into dense projection layer\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "\n",
    "        # Sum up projected input and output + Normalize the layer\n",
    "        out = self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Function for updating parent's config\n",
    "    def get_config(self):\n",
    "\n",
    "        # Get the config of the parent class\n",
    "        config = super().get_config()\n",
    "\n",
    "        # Update the config\n",
    "        config.update({\"embed_dim\": self.embed_dim, \"dense_dim\": self.dense_dim, \"num_heads\": self.num_heads,})\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "# TRANSFORMER DECODER #\n",
    "#######################\n",
    "\n",
    "# Class for transformer decoder\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "\n",
    "        # Inherite parent class constructor\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "\n",
    "        # Initialization\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Multi-head attention layers\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        # Dense projection layers\n",
    "        self.dense_proj = tf.keras.Sequential([tf.keras.layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                                               tf.keras.layers.Dense(embed_dim),])\n",
    "\n",
    "        # Layer normalizations\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        # Flag for masking\n",
    "        self.supports_masking = True\n",
    "\n",
    "    # Call function\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "\n",
    "        # Causal attention mask\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        \n",
    "        # If mask is not None\n",
    "        if mask is not None:\n",
    "\n",
    "            # Mask the inputs\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        # Feed into multi-head attention\n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
    "        \n",
    "        # Sum up inputs and attention + Normalize the layer\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        # Feed into multi-head attention\n",
    "        attention_output_2 = self.attention_2(query=out_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask,)\n",
    "\n",
    "        # Sum up output and attention + Normalize the layer\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        # Feed into dense projection layer\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "\n",
    "        # Sum up output and projected output + Normalize the layer\n",
    "        out = self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Function for getting causal attention mask\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "\n",
    "        # Input shape\n",
    "        input_shape = tf.shape(inputs)\n",
    "\n",
    "        # Batch size AND sequence length\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "\n",
    "        # Range for i and j\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        \n",
    "        # Multiplier (to replicate mask for mult times) \n",
    "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0,)\n",
    "        \n",
    "        # Replicate the mask for mult times \n",
    "        out = tf.tile(mask, mult)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Function for updating parent's config\n",
    "    def get_config(self):\n",
    "\n",
    "        # Get the config of the parent class\n",
    "        config = super().get_config()\n",
    "\n",
    "        # Update the config\n",
    "        config.update({\"embed_dim\": self.embed_dim, \"latent_dim\": self.latent_dim, \"num_heads\": self.num_heads,})\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 128\n",
    "latent_dim = 128\n",
    "num_heads = 4\n",
    "vocab_size = vectorize_layer.vocabulary_size()\n",
    "sequence_length = MAX_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# TRANSFORMER ENCODER #\n",
    "#######################\n",
    "\n",
    "# Inputs\n",
    "encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "# Positional encoding\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "\n",
    "# Transformer encoder\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "\n",
    "# Model\n",
    "encoder = tf.keras.Model(encoder_inputs, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_1 (Positi  (None, None, 128)   10533632    ['decoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_state_inputs (InputLay  [(None, None, 128)]  0          []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 128)   561408      ['positional_embedding_1[0][0]', \n",
      " erDecoder)                                                       'decoder_state_inputs[0][0]']   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 128)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, None, 82244)  10609476    ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,704,516\n",
      "Trainable params: 21,704,516\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# TRANSFORMER DECODER #\n",
    "#######################\n",
    "\n",
    "# Inputs (to decoder)\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "\n",
    "# Inputs (from encoder)\n",
    "encoded_seq_inputs = tf.keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "\n",
    "# Positional encoding\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "\n",
    "# Transformer decoder\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "\n",
    "# Dropout\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "decoder_outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "# Model\n",
    "decoder = tf.keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "# Report\n",
    "print(decoder.summary())\n",
    "# tf.keras.utils.plot_model(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 128)   10533632    ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 128)   297344      ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 82244)  21704516    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,535,492\n",
      "Trainable params: 32,535,492\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#####################\n",
    "# TRANSFORMER MODEL #\n",
    "#####################\n",
    "\n",
    "# Feed inputs into decoder\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "# Transformer model\n",
    "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
    "\n",
    "# Report\n",
    "print(transformer.summary())\n",
    "# tf.keras.utils.plot_model(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3274/3274 [==============================] - 294s 88ms/step - loss: 3.5208 - accuracy: 0.1596 - sparse_categorical_accuracy: 0.1596\n",
      "Epoch 2/5\n",
      "3274/3274 [==============================] - 287s 87ms/step - loss: 3.0473 - accuracy: 0.2117 - sparse_categorical_accuracy: 0.2117\n",
      "Epoch 3/5\n",
      "3274/3274 [==============================] - 286s 87ms/step - loss: 2.7554 - accuracy: 0.2478 - sparse_categorical_accuracy: 0.2478\n",
      "Epoch 4/5\n",
      "3274/3274 [==============================] - 288s 88ms/step - loss: 2.5365 - accuracy: 0.2795 - sparse_categorical_accuracy: 0.2795\n",
      "Epoch 5/5\n",
      "3274/3274 [==============================] - 288s 88ms/step - loss: 2.3645 - accuracy: 0.3077 - sparse_categorical_accuracy: 0.3077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17957627010>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "transformer.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "transformer.fit(ds, epochs=5)      # Choose at least 30 for epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved model/transformer_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved model/transformer_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "transformer.save(\"./saved model/transformer_model\")\n",
    "\n",
    "# Load the model\n",
    "#transformer = tf.keras.models.load_model(\"./saved model/transformer_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Testing and Prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish vocabulary\n",
    "spa_vocab = spa_vectorization.get_vocabulary()\n",
    "\n",
    "# Int2Word dictionary\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "\n",
    "# Set the maximum decoded sequence length \n",
    "max_decoded_sentence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for decoding sequences\n",
    "def decode_sequence(input_sentence):\n",
    "\n",
    "    # Vectorize input english sequence\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    \n",
    "    # Initialize the decoded sentence with [start] token\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    # Loop for max_decoded_sentence_length times\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "\n",
    "        # Vectorize decoded sentence\n",
    "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
    "        \n",
    "        # Predict the [input sequence, target sequence] using transformer\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        # Get the argmax of prediction\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "\n",
    "        # Get the word of the argmax\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "\n",
    "        # Append the word to the decoded sentence\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        # If the sampled token is [end], break the loop\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all english sequences\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "\n",
    "# Predict for N times\n",
    "for _ in range(5):\n",
    "\n",
    "    # Choose a random english-spanish sequence\n",
    "    input_sentence = random.choice(test_pairs)\n",
    "\n",
    "    # Predict the sequence\n",
    "    translated = decode_sequence(input_sentence[0])\n",
    "\n",
    "    # Report\n",
    "    print(\"INPUT:               \", input_sentence[0])\n",
    "    print(\"OUTPUT (TRUE):       \", input_sentence[1])\n",
    "    print(\"OUTPUT (PREDICTION): \", translated, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('prime')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bf5a16b79f3cb90985e3628257d567fbd7649cb1685a1c31906a530b639e1e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
